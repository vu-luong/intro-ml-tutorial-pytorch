{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Đạo hàm tự động với ``torch.autograd``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Vì sao cần đạo hàm?\n",
    "\n",
    "- Mục tiêu: Dùng **Gradient Descent** để tìm min f(x)\n",
    "- Giải thích Gradient Descent: $ x^{(k+1)} = x^{(k)} - \\eta \\times f'(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ví dụ sử dụng Gradient Descent với numpy\n",
    "- Bài toán: tìm cực tiểu hàm $ f(x) = x^2 - 2x + 5 $\n",
    "- Dễ thấy $ min(f) $ = 4 tại $ x = 1 $\n",
    "- Tính đạo hàm thủ công: $ f'(x) = 2x - 2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97731305 0.32254855 0.40530297 0.65032974 0.48677213]\n",
      "[0.98185044 0.45803884 0.52424237 0.72026379 0.5894177 ]\n",
      "[0.98548035 0.56643107 0.6193939  0.77621103 0.67153416]\n",
      "[0.98838428 0.65314486 0.69551512 0.82096883 0.73722733]\n",
      "[0.99070743 0.72251589 0.7564121  0.85677506 0.78978186]\n",
      "[0.99256594 0.77801271 0.80512968 0.88542005 0.83182549]\n",
      "[0.99405275 0.82241017 0.84410374 0.90833604 0.86546039]\n",
      "[0.9952422  0.85792813 0.87528299 0.92666883 0.89236831]\n",
      "[0.99619376 0.88634251 0.90022639 0.94133507 0.91389465]\n",
      "[0.99695501 0.90907401 0.92018112 0.95306805 0.93111572]\n",
      "[0.99756401 0.9272592  0.93614489 0.96245444 0.94489258]\n",
      "[0.99805121 0.94180736 0.94891591 0.96996355 0.95591406]\n",
      "[0.99844096 0.95344589 0.95913273 0.97597084 0.96473125]\n",
      "[0.99875277 0.96275671 0.96730618 0.98077667 0.971785  ]\n",
      "[0.99900222 0.97020537 0.97384495 0.98462134 0.977428  ]\n",
      "[0.99920177 0.9761643  0.97907596 0.98769707 0.9819424 ]\n",
      "[0.99936142 0.98093144 0.98326077 0.99015766 0.98555392]\n",
      "[0.99948914 0.98474515 0.98660861 0.99212613 0.98844314]\n",
      "[0.99959131 0.98779612 0.98928689 0.9937009  0.99075451]\n",
      "[0.99967305 0.9902369  0.99142951 0.99496072 0.99260361]\n",
      "[0.99973844 0.99218952 0.99314361 0.99596858 0.99408289]\n",
      "[0.99979075 0.99375161 0.99451489 0.99677486 0.99526631]\n",
      "[0.9998326  0.99500129 0.99561191 0.99741989 0.99621305]\n",
      "[0.99986608 0.99600103 0.99648953 0.99793591 0.99697044]\n",
      "[0.99989286 0.99680083 0.99719162 0.99834873 0.99757635]\n",
      "[0.99991429 0.99744066 0.9977533  0.99867898 0.99806108]\n",
      "[0.99993143 0.99795253 0.99820264 0.99894319 0.99844886]\n",
      "[0.99994515 0.99836202 0.99856211 0.99915455 0.99875909]\n",
      "[0.99995612 0.99868962 0.99884969 0.99932364 0.99900727]\n",
      "[0.99996489 0.99895169 0.99907975 0.99945891 0.99920582]\n",
      "[0.99997191 0.99916136 0.9992638  0.99956713 0.99936465]\n",
      "[0.99997753 0.99932908 0.99941104 0.9996537  0.99949172]\n",
      "[0.99998203 0.99946327 0.99952883 0.99972296 0.99959338]\n",
      "[0.99998562 0.99957061 0.99962307 0.99977837 0.9996747 ]\n",
      "[0.9999885  0.99965649 0.99969845 0.9998227  0.99973976]\n",
      "[0.9999908  0.99972519 0.99975876 0.99985816 0.99979181]\n",
      "[0.99999264 0.99978015 0.99980701 0.99988653 0.99983345]\n",
      "[0.99999411 0.99982412 0.99984561 0.99990922 0.99986676]\n",
      "[0.99999529 0.9998593  0.99987649 0.99992738 0.99989341]\n",
      "[0.99999623 0.99988744 0.99990119 0.9999419  0.99991473]\n",
      "[0.99999698 0.99990995 0.99992095 0.99995352 0.99993178]\n",
      "[0.99999759 0.99992796 0.99993676 0.99996282 0.99994542]\n",
      "[0.99999807 0.99994237 0.99994941 0.99997025 0.99995634]\n",
      "[0.99999846 0.9999539  0.99995953 0.9999762  0.99996507]\n",
      "[0.99999876 0.99996312 0.99996762 0.99998096 0.99997206]\n",
      "[0.99999901 0.99997049 0.9999741  0.99998477 0.99997765]\n",
      "[0.99999921 0.99997639 0.99997928 0.99998782 0.99998212]\n",
      "[0.99999937 0.99998112 0.99998342 0.99999025 0.99998569]\n",
      "[0.99999949 0.99998489 0.99998674 0.9999922  0.99998855]\n",
      "[0.9999996  0.99998791 0.99998939 0.99999376 0.99999084]\n",
      "[0.99999968 0.99999033 0.99999151 0.99999501 0.99999267]\n",
      "[0.99999974 0.99999226 0.99999321 0.99999601 0.99999414]\n",
      "[0.99999979 0.99999381 0.99999457 0.99999681 0.99999531]\n",
      "[0.99999983 0.99999505 0.99999565 0.99999744 0.99999625]\n",
      "[0.99999987 0.99999604 0.99999652 0.99999796 0.999997  ]\n",
      "[0.99999989 0.99999683 0.99999722 0.99999836 0.9999976 ]\n",
      "[0.99999992 0.99999747 0.99999777 0.99999869 0.99999808]\n",
      "[0.99999993 0.99999797 0.99999822 0.99999895 0.99999846]\n",
      "[0.99999995 0.99999838 0.99999858 0.99999916 0.99999877]\n",
      "[0.99999996 0.9999987  0.99999886 0.99999933 0.99999902]\n",
      "[0.99999997 0.99999896 0.99999909 0.99999946 0.99999921]\n",
      "[0.99999997 0.99999917 0.99999927 0.99999957 0.99999937]\n",
      "[0.99999998 0.99999934 0.99999942 0.99999966 0.9999995 ]\n",
      "[0.99999998 0.99999947 0.99999953 0.99999973 0.9999996 ]\n",
      "[0.99999999 0.99999957 0.99999963 0.99999978 0.99999968]\n",
      "[0.99999999 0.99999966 0.9999997  0.99999982 0.99999974]\n",
      "[0.99999999 0.99999973 0.99999976 0.99999986 0.99999979]\n",
      "[0.99999999 0.99999978 0.99999981 0.99999989 0.99999984]\n",
      "[0.99999999 0.99999983 0.99999985 0.99999991 0.99999987]\n",
      "[1.         0.99999986 0.99999988 0.99999993 0.99999989]\n",
      "[1.         0.99999989 0.9999999  0.99999994 0.99999992]\n",
      "[1.         0.99999991 0.99999992 0.99999995 0.99999993]\n",
      "[1.         0.99999993 0.99999994 0.99999996 0.99999995]\n",
      "[1.         0.99999994 0.99999995 0.99999997 0.99999996]\n",
      "[1.         0.99999995 0.99999996 0.99999998 0.99999997]\n",
      "[1.         0.99999996 0.99999997 0.99999998 0.99999997]\n",
      "[1.         0.99999997 0.99999997 0.99999998 0.99999998]\n",
      "[1.         0.99999998 0.99999998 0.99999999 0.99999998]\n",
      "[1.         0.99999998 0.99999998 0.99999999 0.99999999]\n",
      "[1.         0.99999999 0.99999999 0.99999999 0.99999999]\n",
      "[1.         0.99999999 0.99999999 0.99999999 0.99999999]\n",
      "[1.         0.99999999 0.99999999 1.         0.99999999]\n",
      "[1.         0.99999999 0.99999999 1.         0.99999999]\n",
      "[1.         0.99999999 0.99999999 1.         1.        ]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Dùng code\n",
    "learning_rate = 0.1\n",
    "x_0 = np.random.rand(5)\n",
    "for iter in range(100):\n",
    "    x_0 = x_0 - learning_rate*(2*x_0 - 2)\n",
    "    print(x_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ví dụ sử dụng Gradient Descent với pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Tính đạo hàm bằng pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative by pytorch:  tensor([-0.6214])\n",
      "Manual Derivative:  tensor([-0.6214], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1)\n",
    "x.requires_grad_(True)\n",
    "f = x*x - 2*x + 5\n",
    "f.backward()\n",
    "print('Derivative by pytorch: ', x.grad)\n",
    "print('Manual Derivative: ', 2*x - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Gradient Descent với pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x*x - 2*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7775])\n",
      "tensor([0.8220])\n",
      "tensor([0.8576])\n",
      "tensor([0.8861])\n",
      "tensor([0.9088])\n",
      "tensor([0.9271])\n",
      "tensor([0.9417])\n",
      "tensor([0.9533])\n",
      "tensor([0.9627])\n",
      "tensor([0.9701])\n",
      "tensor([0.9761])\n",
      "tensor([0.9809])\n",
      "tensor([0.9847])\n",
      "tensor([0.9878])\n",
      "tensor([0.9902])\n",
      "tensor([0.9922])\n",
      "tensor([0.9937])\n",
      "tensor([0.9950])\n",
      "tensor([0.9960])\n",
      "tensor([0.9968])\n",
      "tensor([0.9974])\n",
      "tensor([0.9979])\n",
      "tensor([0.9984])\n",
      "tensor([0.9987])\n",
      "tensor([0.9989])\n",
      "tensor([0.9992])\n",
      "tensor([0.9993])\n",
      "tensor([0.9995])\n",
      "tensor([0.9996])\n",
      "tensor([0.9997])\n",
      "tensor([0.9997])\n",
      "tensor([0.9998])\n",
      "tensor([0.9998])\n",
      "tensor([0.9999])\n",
      "tensor([0.9999])\n",
      "tensor([0.9999])\n",
      "tensor([0.9999])\n",
      "tensor([0.9999])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n",
      "tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "x = torch.rand(1, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x], lr=0.1)\n",
    "\n",
    "for iter in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    out = f(x)\n",
    "    out.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    print(x.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
